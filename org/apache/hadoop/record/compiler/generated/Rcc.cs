/* Generated By:JavaCC: Do not edit this line. Rcc.java */
using Sharpen;

namespace org.apache.hadoop.record.compiler.generated
{
	[System.ObsoleteAttribute(@"Replaced by <a href=""http://hadoop.apache.org/avro/"">Avro</a>."
		)]
	public class Rcc : org.apache.hadoop.record.compiler.generated.RccConstants
	{
		private static string language = "java";

		private static string destDir = ".";

		private static System.Collections.Generic.List<string> recFiles = new System.Collections.Generic.List
			<string>();

		private static System.Collections.Generic.List<string> cmdargs = new System.Collections.Generic.List
			<string>();

		private static org.apache.hadoop.record.compiler.JFile curFile;

		private static java.util.Hashtable<string, org.apache.hadoop.record.compiler.JRecord
			> recTab;

		private static string curDir = ".";

		private static string curFileName;

		private static string curModuleName;

		public static void Main(string[] args)
		{
			System.Environment.Exit(driver(args));
		}

		public static void usage()
		{
			System.Console.Error.WriteLine("Usage: rcc --language [java|c++] ddl-files");
		}

		public static int driver(string[] args)
		{
			for (int i = 0; i < args.Length; i++)
			{
				if (Sharpen.Runtime.equalsIgnoreCase("-l", args[i]) || Sharpen.Runtime.equalsIgnoreCase
					("--language", args[i]))
				{
					language = org.apache.hadoop.util.StringUtils.toLowerCase(args[i + 1]);
					i++;
				}
				else
				{
					if (Sharpen.Runtime.equalsIgnoreCase("-d", args[i]) || Sharpen.Runtime.equalsIgnoreCase
						("--destdir", args[i]))
					{
						destDir = args[i + 1];
						i++;
					}
					else
					{
						if (args[i].StartsWith("-"))
						{
							string arg = Sharpen.Runtime.substring(args[i], 1);
							if (arg.StartsWith("-"))
							{
								arg = Sharpen.Runtime.substring(arg, 1);
							}
							cmdargs.add(org.apache.hadoop.util.StringUtils.toLowerCase(arg));
						}
						else
						{
							recFiles.add(args[i]);
						}
					}
				}
			}
			if (recFiles.Count == 0)
			{
				usage();
				return 1;
			}
			for (int i_1 = 0; i_1 < recFiles.Count; i_1++)
			{
				curFileName = recFiles[i_1];
				java.io.File file = new java.io.File(curFileName);
				try
				{
					java.io.FileReader reader = new java.io.FileReader(file);
					org.apache.hadoop.record.compiler.generated.Rcc parser = new org.apache.hadoop.record.compiler.generated.Rcc
						(reader);
					try
					{
						recTab = new java.util.Hashtable<string, org.apache.hadoop.record.compiler.JRecord
							>();
						curFile = parser.Input();
					}
					catch (org.apache.hadoop.record.compiler.generated.ParseException e)
					{
						System.Console.Error.WriteLine(e.ToString());
						return 1;
					}
					try
					{
						reader.close();
					}
					catch (System.IO.IOException)
					{
					}
				}
				catch (java.io.FileNotFoundException)
				{
					System.Console.Error.WriteLine("File " + recFiles[i_1] + " Not found.");
					return 1;
				}
				try
				{
					int retCode = curFile.genCode(language, destDir, cmdargs);
					if (retCode != 0)
					{
						return retCode;
					}
				}
				catch (System.IO.IOException e)
				{
					System.Console.Error.WriteLine(e.ToString());
					return 1;
				}
			}
			return 0;
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JFile Input()
		{
			System.Collections.Generic.List<org.apache.hadoop.record.compiler.JFile> ilist = 
				new System.Collections.Generic.List<org.apache.hadoop.record.compiler.JFile>();
			System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord> rlist = 
				new System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord>();
			org.apache.hadoop.record.compiler.JFile i;
			System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord> l;
			while (true)
			{
				switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk)
				{
					case INCLUDE_TKN:
					{
						i = Include();
						ilist.add(i);
						break;
					}

					case MODULE_TKN:
					{
						l = Module();
						Sharpen.Collections.AddAll(rlist, l);
						break;
					}

					default:
					{
						jj_la1[0] = jj_gen;
						jj_consume_token(-1);
						throw new org.apache.hadoop.record.compiler.generated.ParseException();
					}
				}
				switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk)
				{
					case MODULE_TKN:
					case INCLUDE_TKN:
					{
						break;
					}

					default:
					{
						jj_la1[1] = jj_gen;
						goto label_1_break;
					}
				}
label_1_continue: ;
			}
label_1_break: ;
			jj_consume_token(0);
			{
				if (true)
				{
					return new org.apache.hadoop.record.compiler.JFile(curFileName, ilist, rlist);
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JFile Include()
		{
			string fname;
			org.apache.hadoop.record.compiler.generated.Token t;
			jj_consume_token(INCLUDE_TKN);
			t = jj_consume_token(CSTRING_TKN);
			org.apache.hadoop.record.compiler.JFile ret = null;
			fname = t.image.replaceAll("^\"", string.Empty).replaceAll("\"$", string.Empty);
			java.io.File file = new java.io.File(curDir, fname);
			string tmpDir = curDir;
			string tmpFile = curFileName;
			curDir = file.getParent();
			curFileName = file.getName();
			try
			{
				java.io.FileReader reader = new java.io.FileReader(file);
				org.apache.hadoop.record.compiler.generated.Rcc parser = new org.apache.hadoop.record.compiler.generated.Rcc
					(reader);
				try
				{
					ret = parser.Input();
					System.Console.Out.WriteLine(fname + " Parsed Successfully");
				}
				catch (org.apache.hadoop.record.compiler.generated.ParseException e)
				{
					System.Console.Out.WriteLine(e.ToString());
					System.Environment.Exit(1);
				}
				try
				{
					reader.close();
				}
				catch (System.IO.IOException)
				{
				}
			}
			catch (java.io.FileNotFoundException)
			{
				System.Console.Out.WriteLine("File " + fname + " Not found.");
				System.Environment.Exit(1);
			}
			curDir = tmpDir;
			curFileName = tmpFile;
			{
				if (true)
				{
					return ret;
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord>
			 Module()
		{
			string mName;
			System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord> rlist;
			jj_consume_token(MODULE_TKN);
			mName = ModuleName();
			curModuleName = mName;
			jj_consume_token(LBRACE_TKN);
			rlist = RecordList();
			jj_consume_token(RBRACE_TKN);
			{
				if (true)
				{
					return rlist;
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public string ModuleName()
		{
			string name = string.Empty;
			org.apache.hadoop.record.compiler.generated.Token t;
			t = jj_consume_token(IDENT_TKN);
			name += t.image;
			while (true)
			{
				switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk)
				{
					case DOT_TKN:
					{
						break;
					}

					default:
					{
						jj_la1[2] = jj_gen;
						goto label_2_break;
					}
				}
				jj_consume_token(DOT_TKN);
				t = jj_consume_token(IDENT_TKN);
				name += "." + t.image;
label_2_continue: ;
			}
label_2_break: ;
			{
				if (true)
				{
					return name;
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord>
			 RecordList()
		{
			System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord> rlist = 
				new System.Collections.Generic.List<org.apache.hadoop.record.compiler.JRecord>();
			org.apache.hadoop.record.compiler.JRecord r;
			while (true)
			{
				r = Record();
				rlist.add(r);
				switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk)
				{
					case RECORD_TKN:
					{
						break;
					}

					default:
					{
						jj_la1[3] = jj_gen;
						goto label_3_break;
					}
				}
label_3_continue: ;
			}
label_3_break: ;
			{
				if (true)
				{
					return rlist;
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JRecord Record()
		{
			string rname;
			System.Collections.Generic.List<org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType
				>> flist = new System.Collections.Generic.List<org.apache.hadoop.record.compiler.JField
				<org.apache.hadoop.record.compiler.JType>>();
			org.apache.hadoop.record.compiler.generated.Token t;
			org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType>
				 f;
			jj_consume_token(RECORD_TKN);
			t = jj_consume_token(IDENT_TKN);
			rname = t.image;
			jj_consume_token(LBRACE_TKN);
			while (true)
			{
				f = Field();
				flist.add(f);
				jj_consume_token(SEMICOLON_TKN);
				switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk)
				{
					case BYTE_TKN:
					case BOOLEAN_TKN:
					case INT_TKN:
					case LONG_TKN:
					case FLOAT_TKN:
					case DOUBLE_TKN:
					case USTRING_TKN:
					case BUFFER_TKN:
					case VECTOR_TKN:
					case MAP_TKN:
					case IDENT_TKN:
					{
						break;
					}

					default:
					{
						jj_la1[4] = jj_gen;
						goto label_4_break;
					}
				}
label_4_continue: ;
			}
label_4_break: ;
			jj_consume_token(RBRACE_TKN);
			string fqn = curModuleName + "." + rname;
			org.apache.hadoop.record.compiler.JRecord r = new org.apache.hadoop.record.compiler.JRecord
				(fqn, flist);
			recTab[fqn] = r;
			{
				if (true)
				{
					return r;
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType
			> Field()
		{
			org.apache.hadoop.record.compiler.JType jt;
			org.apache.hadoop.record.compiler.generated.Token t;
			jt = Type();
			t = jj_consume_token(IDENT_TKN);
			{
				if (true)
				{
					return new org.apache.hadoop.record.compiler.JField<org.apache.hadoop.record.compiler.JType
						>(t.image, jt);
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JType Type()
		{
			org.apache.hadoop.record.compiler.JType jt;
			org.apache.hadoop.record.compiler.generated.Token t;
			string rname;
			switch ((jj_ntk == -1) ? jj_ntk() : jj_ntk)
			{
				case MAP_TKN:
				{
					jt = Map();
					if (true)
					{
						return jt;
					}
					break;
				}

				case VECTOR_TKN:
				{
					jt = Vector();
					if (true)
					{
						return jt;
					}
					break;
				}

				case BYTE_TKN:
				{
					jj_consume_token(BYTE_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JByte();
					}
					break;
				}

				case BOOLEAN_TKN:
				{
					jj_consume_token(BOOLEAN_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JBoolean();
					}
					break;
				}

				case INT_TKN:
				{
					jj_consume_token(INT_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JInt();
					}
					break;
				}

				case LONG_TKN:
				{
					jj_consume_token(LONG_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JLong();
					}
					break;
				}

				case FLOAT_TKN:
				{
					jj_consume_token(FLOAT_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JFloat();
					}
					break;
				}

				case DOUBLE_TKN:
				{
					jj_consume_token(DOUBLE_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JDouble();
					}
					break;
				}

				case USTRING_TKN:
				{
					jj_consume_token(USTRING_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JString();
					}
					break;
				}

				case BUFFER_TKN:
				{
					jj_consume_token(BUFFER_TKN);
					if (true)
					{
						return new org.apache.hadoop.record.compiler.JBuffer();
					}
					break;
				}

				case IDENT_TKN:
				{
					rname = ModuleName();
					if (rname.IndexOf('.', 0) < 0)
					{
						rname = curModuleName + "." + rname;
					}
					org.apache.hadoop.record.compiler.JRecord r = recTab[rname];
					if (r == null)
					{
						System.Console.Out.WriteLine("Type " + rname + " not known. Exiting.");
						System.Environment.Exit(1);
					}
					if (true)
					{
						return r;
					}
					break;
				}

				default:
				{
					jj_la1[5] = jj_gen;
					jj_consume_token(-1);
					throw new org.apache.hadoop.record.compiler.generated.ParseException();
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JMap Map()
		{
			org.apache.hadoop.record.compiler.JType jt1;
			org.apache.hadoop.record.compiler.JType jt2;
			jj_consume_token(MAP_TKN);
			jj_consume_token(LT_TKN);
			jt1 = Type();
			jj_consume_token(COMMA_TKN);
			jt2 = Type();
			jj_consume_token(GT_TKN);
			{
				if (true)
				{
					return new org.apache.hadoop.record.compiler.JMap(jt1, jt2);
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		public org.apache.hadoop.record.compiler.JVector Vector()
		{
			org.apache.hadoop.record.compiler.JType jt;
			jj_consume_token(VECTOR_TKN);
			jj_consume_token(LT_TKN);
			jt = Type();
			jj_consume_token(GT_TKN);
			{
				if (true)
				{
					return new org.apache.hadoop.record.compiler.JVector(jt);
				}
			}
			throw new System.Exception("Missing return statement in function");
		}

		public org.apache.hadoop.record.compiler.generated.RccTokenManager token_source;

		internal org.apache.hadoop.record.compiler.generated.SimpleCharStream jj_input_stream;

		public org.apache.hadoop.record.compiler.generated.Token token;

		public org.apache.hadoop.record.compiler.generated.Token jj_nt;

		private int jj_ntk;

		private int jj_gen;

		private readonly int[] jj_la1 = new int[6];

		private static int[] jj_la1_0;

		private static int[] jj_la1_1;

		static Rcc()
		{
			jj_la1_0();
			jj_la1_1();
		}

		private static void jj_la1_0()
		{
			jj_la1_0 = new int[] { unchecked((int)(0x2800)), unchecked((int)(0x2800)), unchecked(
				(int)(0x40000000)), unchecked((int)(0x1000)), unchecked((int)(0xffc000)), unchecked(
				(int)(0xffc000)) };
		}

		private static void jj_la1_1()
		{
			jj_la1_1 = new int[] { unchecked((int)(0x0)), unchecked((int)(0x0)), unchecked((int
				)(0x0)), unchecked((int)(0x0)), unchecked((int)(0x1)), unchecked((int)(0x1)) };
		}

		public Rcc(java.io.InputStream stream)
			: this(stream, null)
		{
		}

		public Rcc(java.io.InputStream stream, string encoding)
		{
			try
			{
				jj_input_stream = new org.apache.hadoop.record.compiler.generated.SimpleCharStream
					(stream, encoding, 1, 1);
			}
			catch (java.io.UnsupportedEncodingException e)
			{
				throw new System.Exception(e);
			}
			token_source = new org.apache.hadoop.record.compiler.generated.RccTokenManager(jj_input_stream
				);
			token = new org.apache.hadoop.record.compiler.generated.Token();
			jj_ntk = -1;
			jj_gen = 0;
			for (int i = 0; i < 6; i++)
			{
				jj_la1[i] = -1;
			}
		}

		public virtual void ReInit(java.io.InputStream stream)
		{
			ReInit(stream, null);
		}

		public virtual void ReInit(java.io.InputStream stream, string encoding)
		{
			try
			{
				jj_input_stream.ReInit(stream, encoding, 1, 1);
			}
			catch (java.io.UnsupportedEncodingException e)
			{
				throw new System.Exception(e);
			}
			token_source.ReInit(jj_input_stream);
			token = new org.apache.hadoop.record.compiler.generated.Token();
			jj_ntk = -1;
			jj_gen = 0;
			for (int i = 0; i < 6; i++)
			{
				jj_la1[i] = -1;
			}
		}

		public Rcc(java.io.Reader stream)
		{
			jj_input_stream = new org.apache.hadoop.record.compiler.generated.SimpleCharStream
				(stream, 1, 1);
			token_source = new org.apache.hadoop.record.compiler.generated.RccTokenManager(jj_input_stream
				);
			token = new org.apache.hadoop.record.compiler.generated.Token();
			jj_ntk = -1;
			jj_gen = 0;
			for (int i = 0; i < 6; i++)
			{
				jj_la1[i] = -1;
			}
		}

		public virtual void ReInit(java.io.Reader stream)
		{
			jj_input_stream.ReInit(stream, 1, 1);
			token_source.ReInit(jj_input_stream);
			token = new org.apache.hadoop.record.compiler.generated.Token();
			jj_ntk = -1;
			jj_gen = 0;
			for (int i = 0; i < 6; i++)
			{
				jj_la1[i] = -1;
			}
		}

		public Rcc(org.apache.hadoop.record.compiler.generated.RccTokenManager tm)
		{
			token_source = tm;
			token = new org.apache.hadoop.record.compiler.generated.Token();
			jj_ntk = -1;
			jj_gen = 0;
			for (int i = 0; i < 6; i++)
			{
				jj_la1[i] = -1;
			}
		}

		public virtual void ReInit(org.apache.hadoop.record.compiler.generated.RccTokenManager
			 tm)
		{
			token_source = tm;
			token = new org.apache.hadoop.record.compiler.generated.Token();
			jj_ntk = -1;
			jj_gen = 0;
			for (int i = 0; i < 6; i++)
			{
				jj_la1[i] = -1;
			}
		}

		/// <exception cref="org.apache.hadoop.record.compiler.generated.ParseException"/>
		private org.apache.hadoop.record.compiler.generated.Token jj_consume_token(int kind
			)
		{
			org.apache.hadoop.record.compiler.generated.Token oldToken;
			if ((oldToken = token).next != null)
			{
				token = token.next;
			}
			else
			{
				token = token.next = token_source.getNextToken();
			}
			jj_ntk = -1;
			if (token.kind == kind)
			{
				jj_gen++;
				return token;
			}
			token = oldToken;
			jj_kind = kind;
			throw generateParseException();
		}

		public org.apache.hadoop.record.compiler.generated.Token getNextToken()
		{
			if (token.next != null)
			{
				token = token.next;
			}
			else
			{
				token = token.next = token_source.getNextToken();
			}
			jj_ntk = -1;
			jj_gen++;
			return token;
		}

		public org.apache.hadoop.record.compiler.generated.Token getToken(int index)
		{
			org.apache.hadoop.record.compiler.generated.Token t = token;
			for (int i = 0; i < index; i++)
			{
				if (t.next != null)
				{
					t = t.next;
				}
				else
				{
					t = t.next = token_source.getNextToken();
				}
			}
			return t;
		}

		private int jj_ntk()
		{
			if ((jj_nt = token.next) == null)
			{
				return (jj_ntk = (token.next = token_source.getNextToken()).kind);
			}
			else
			{
				return (jj_ntk = jj_nt.kind);
			}
		}

		private java.util.Vector<int[]> jj_expentries = new java.util.Vector<int[]>();

		private int[] jj_expentry;

		private int jj_kind = -1;

		public virtual org.apache.hadoop.record.compiler.generated.ParseException generateParseException
			()
		{
			jj_expentries.Clear();
			bool[] la1tokens = new bool[33];
			for (int i = 0; i < 33; i++)
			{
				la1tokens[i] = false;
			}
			if (jj_kind >= 0)
			{
				la1tokens[jj_kind] = true;
				jj_kind = -1;
			}
			for (int i_1 = 0; i_1 < 6; i_1++)
			{
				if (jj_la1[i_1] == jj_gen)
				{
					for (int j = 0; j < 32; j++)
					{
						if ((jj_la1_0[i_1] & (1 << j)) != 0)
						{
							la1tokens[j] = true;
						}
						if ((jj_la1_1[i_1] & (1 << j)) != 0)
						{
							la1tokens[32 + j] = true;
						}
					}
				}
			}
			for (int i_2 = 0; i_2 < 33; i_2++)
			{
				if (la1tokens[i_2])
				{
					jj_expentry = new int[1];
					jj_expentry[0] = i_2;
					jj_expentries.Add(jj_expentry);
				}
			}
			int[][] exptokseq = new int[jj_expentries.Count][];
			for (int i_3 = 0; i_3 < jj_expentries.Count; i_3++)
			{
				exptokseq[i_3] = jj_expentries[i_3];
			}
			return new org.apache.hadoop.record.compiler.generated.ParseException(token, exptokseq
				, tokenImage);
		}

		public void enable_tracing()
		{
		}

		public void disable_tracing()
		{
		}
	}
}
