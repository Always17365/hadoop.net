using Sharpen;

namespace org.apache.hadoop.fs.loadGenerator
{
	/// <summary>
	/// The load generator is a tool for testing NameNode behavior under
	/// different client loads.
	/// </summary>
	/// <remarks>
	/// The load generator is a tool for testing NameNode behavior under
	/// different client loads. Note there is a subclass of this clas that lets
	/// you run a the load generator as a MapReduce job (see LoadGeneratorMR in the
	/// MapReduce project.
	/// The loadGenerator allows the user to generate different mixes of read, write,
	/// and list requests by specifying the probabilities of read and
	/// write. The user controls the intensity of the load by
	/// adjusting parameters for the number of worker threads and the delay
	/// between operations. While load generators are running, the user
	/// can profile and monitor the running of the NameNode. When a load
	/// generator exits, it print some NameNode statistics like the average
	/// execution time of each kind of operations and the NameNode
	/// throughput.
	/// The program can run in one of two forms. As a regular single process command
	/// that runs multiple threads to generate load on the NN or as a Map Reduce
	/// program that runs multiple (multi-threaded) map tasks that generate load
	/// on the NN; the results summary is generated by a single reduce task.
	/// The user may either specify constant duration, read and write
	/// probabilities via the command line, or may specify a text file
	/// that acts as a script of which read and write probabilities to
	/// use for specified durations. If no duration is specified the program
	/// runs till killed (duration required if run as MapReduce).
	/// The script takes the form of lines of duration in seconds, read
	/// probability and write probability, each separated by white space.
	/// Blank lines and lines starting with # (comments) are ignored. If load
	/// generator is run as a MapReduce program then the script file needs to be
	/// accessible on the the Map task as a HDFS file.
	/// After command line argument parsing and data initialization,
	/// the load generator spawns the number of worker threads
	/// as specified by the user.
	/// Each thread sends a stream of requests to the NameNode.
	/// For each iteration, it first decides if it is going to read a file,
	/// create a file, or listing a directory following the read and write
	/// probabilities specified by the user.
	/// When reading, it randomly picks a file in the test space and reads
	/// the entire file. When writing, it randomly picks a directory in the
	/// test space and creates a file whose name consists of the current
	/// machine's host name and the thread id. The length of the file
	/// follows Gaussian distribution with an average size of 2 blocks and
	/// the standard deviation of 1 block. The new file is filled with 'a'.
	/// Immediately after the file creation completes, the file is deleted
	/// from the test space.
	/// While listing, it randomly picks a directory in the test space and
	/// list the directory content.
	/// Between two consecutive operations, the thread pauses for a random
	/// amount of time in the range of [0, maxDelayBetweenOps]
	/// if the specified max delay is not zero.
	/// All threads are stopped when the specified elapsed time has passed
	/// in command-line execution, or all the lines of script have been
	/// executed, if using a script.
	/// Before exiting, the program prints the average execution for
	/// each kind of NameNode operations, and the number of requests
	/// served by the NameNode.
	/// The synopsis of the command is
	/// java LoadGenerator
	/// -readProbability <read probability>: read probability [0, 1]
	/// with a default value of 0.3333.
	/// -writeProbability <write probability>: write probability [0, 1]
	/// with a default value of 0.3333.
	/// -root <root>: test space with a default value of /testLoadSpace
	/// -maxDelayBetweenOps <maxDelayBetweenOpsInMillis>:
	/// Max delay in the unit of milliseconds between two operations with a
	/// default value of 0 indicating no delay.
	/// -numOfThreads <numOfThreads>:
	/// number of threads to spawn with a default value of 200.
	/// -elapsedTime <elapsedTimeInSecs>:
	/// the elapsed time of program with a default value of 0
	/// indicating running forever
	/// -startTime <startTimeInMillis> : when the threads start to run.
	/// -scriptFile <file name>: text file to parse for scripted operation
	/// </remarks>
	public class LoadGenerator : org.apache.hadoop.conf.Configured, org.apache.hadoop.util.Tool
	{
		public static readonly org.apache.commons.logging.Log LOG = org.apache.commons.logging.LogFactory
			.getLog(Sharpen.Runtime.getClassForType(typeof(org.apache.hadoop.fs.loadGenerator.LoadGenerator
			)));

		private static volatile bool shouldRun = true;

		protected internal static org.apache.hadoop.fs.Path root = org.apache.hadoop.fs.loadGenerator.DataGenerator
			.DEFAULT_ROOT;

		private static org.apache.hadoop.fs.FileContext fc;

		protected internal static int maxDelayBetweenOps = 0;

		protected internal static int numOfThreads = 200;

		protected internal static long[] durations = new long[] { 0 };

		protected internal static double[] readProbs = new double[] { 0.3333 };

		protected internal static double[] writeProbs = new double[] { 0.3333 };

		private static volatile int currentIndex = 0;

		protected internal static long totalTime = 0;

		protected internal static long startTime = org.apache.hadoop.util.Time.now() + 10000;

		private const int BLOCK_SIZE = 10;

		private static System.Collections.Generic.List<string> files = new System.Collections.Generic.List
			<string>();

		private static System.Collections.Generic.List<string> dirs = new System.Collections.Generic.List
			<string>();

		protected internal static java.util.Random r = null;

		protected internal static long seed = 0;

		protected internal static string scriptFile = null;

		protected internal const string FLAGFILE_DEFAULT = "/tmp/flagFile";

		protected internal static org.apache.hadoop.fs.Path flagFile = new org.apache.hadoop.fs.Path
			(FLAGFILE_DEFAULT);

		protected internal string hostname;

		private const string USAGE_CMD = "java LoadGenerator\n";

		protected internal const string USAGE_ARGS = "-readProbability <read probability>\n"
			 + "-writeProbability <write probability>\n" + "-root <root>\n" + "-maxDelayBetweenOps <maxDelayBetweenOpsInMillis>\n"
			 + "-numOfThreads <numOfThreads>\n" + "-elapsedTime <elapsedTimeInSecs>\n" + "-startTime <startTimeInMillis>\n"
			 + "-scriptFile <filename>\n" + "-flagFile <filename>";

		private const string USAGE = USAGE_CMD + USAGE_ARGS;

		private readonly byte[] WRITE_CONTENTS = new byte[4096];

		private const int ERR_TEST_FAILED = 2;

		/// <summary>Constructor</summary>
		/// <exception cref="System.IO.IOException"/>
		/// <exception cref="java.net.UnknownHostException"/>
		public LoadGenerator()
		{
			// a table of file names
			// a table of directory names
			java.net.InetAddress addr = java.net.InetAddress.getLocalHost();
			hostname = addr.getHostName();
			java.util.Arrays.fill(WRITE_CONTENTS, unchecked((byte)'a'));
		}

		/// <exception cref="System.IO.IOException"/>
		/// <exception cref="java.net.UnknownHostException"/>
		public LoadGenerator(org.apache.hadoop.conf.Configuration conf)
			: this()
		{
			setConf(conf);
		}

		protected internal const int OPEN = 0;

		protected internal const int LIST = 1;

		protected internal const int CREATE = 2;

		protected internal const int WRITE_CLOSE = 3;

		protected internal const int DELETE = 4;

		protected internal const int TOTAL_OP_TYPES = 5;

		protected internal static long[] executionTime = new long[TOTAL_OP_TYPES];

		protected internal static long[] numOfOps = new long[TOTAL_OP_TYPES];

		protected internal static long totalOps = 0;

		/// <summary>A thread sends a stream of requests to the NameNode.</summary>
		/// <remarks>
		/// A thread sends a stream of requests to the NameNode.
		/// At each iteration, it first decides if it is going to read a file,
		/// create a file, or listing a directory following the read
		/// and write probabilities.
		/// When reading, it randomly picks a file in the test space and reads
		/// the entire file. When writing, it randomly picks a directory in the
		/// test space and creates a file whose name consists of the current
		/// machine's host name and the thread id. The length of the file
		/// follows Gaussian distribution with an average size of 2 blocks and
		/// the standard deviation of 1 block. The new file is filled with 'a'.
		/// Immediately after the file creation completes, the file is deleted
		/// from the test space.
		/// While listing, it randomly picks a directory in the test space and
		/// list the directory content.
		/// Between two consecutive operations, the thread pauses for a random
		/// amount of time in the range of [0, maxDelayBetweenOps]
		/// if the specified max delay is not zero.
		/// A thread runs for the specified elapsed time if the time isn't zero.
		/// Otherwise, it runs forever.
		/// </remarks>
		private class DFSClientThread : java.lang.Thread
		{
			private int id;

			private long[] executionTime = new long[org.apache.hadoop.fs.loadGenerator.LoadGenerator
				.TOTAL_OP_TYPES];

			private long[] totalNumOfOps = new long[org.apache.hadoop.fs.loadGenerator.LoadGenerator
				.TOTAL_OP_TYPES];

			private byte[] buffer = new byte[1024];

			private bool failed;

			private DFSClientThread(LoadGenerator _enclosing, int id)
			{
				this._enclosing = _enclosing;
				// across all of types
				this.id = id;
			}

			/// <summary>
			/// Main loop for each thread
			/// Each iteration decides what's the next operation and then pauses.
			/// </summary>
			public override void run()
			{
				try
				{
					while (org.apache.hadoop.fs.loadGenerator.LoadGenerator.shouldRun)
					{
						this.nextOp();
						this.delay();
					}
				}
				catch (System.Exception ioe)
				{
					System.Console.Error.WriteLine(ioe.getLocalizedMessage());
					Sharpen.Runtime.printStackTrace(ioe);
					this.failed = true;
				}
			}

			/// <summary>
			/// Let the thread pause for a random amount of time in the range of
			/// [0, maxDelayBetweenOps] if the delay is not zero.
			/// </summary>
			/// <remarks>
			/// Let the thread pause for a random amount of time in the range of
			/// [0, maxDelayBetweenOps] if the delay is not zero. Otherwise, no pause.
			/// </remarks>
			/// <exception cref="System.Exception"/>
			private void delay()
			{
				if (org.apache.hadoop.fs.loadGenerator.LoadGenerator.maxDelayBetweenOps > 0)
				{
					int delay = org.apache.hadoop.fs.loadGenerator.LoadGenerator.r.nextInt(org.apache.hadoop.fs.loadGenerator.LoadGenerator
						.maxDelayBetweenOps);
					java.lang.Thread.sleep(delay);
				}
			}

			/// <summary>Perform the next operation.</summary>
			/// <remarks>
			/// Perform the next operation.
			/// Depending on the read and write probabilities, the next
			/// operation could be either read, write, or list.
			/// </remarks>
			/// <exception cref="System.IO.IOException"/>
			private void nextOp()
			{
				double rn = org.apache.hadoop.fs.loadGenerator.LoadGenerator.r.nextDouble();
				int i = org.apache.hadoop.fs.loadGenerator.LoadGenerator.currentIndex;
				if (org.apache.hadoop.fs.loadGenerator.LoadGenerator.LOG.isDebugEnabled())
				{
					org.apache.hadoop.fs.loadGenerator.LoadGenerator.LOG.debug("Thread " + this.id + 
						" moving to index " + i);
				}
				if (rn < org.apache.hadoop.fs.loadGenerator.LoadGenerator.readProbs[i])
				{
					this.read();
				}
				else
				{
					if (rn < org.apache.hadoop.fs.loadGenerator.LoadGenerator.readProbs[i] + org.apache.hadoop.fs.loadGenerator.LoadGenerator
						.writeProbs[i])
					{
						this.write();
					}
					else
					{
						this.list();
					}
				}
			}

			/// <summary>
			/// Read operation randomly picks a file in the test space and reads
			/// the entire file
			/// </summary>
			/// <exception cref="System.IO.IOException"/>
			private void read()
			{
				string fileName = org.apache.hadoop.fs.loadGenerator.LoadGenerator.files[org.apache.hadoop.fs.loadGenerator.LoadGenerator
					.r.nextInt(org.apache.hadoop.fs.loadGenerator.LoadGenerator.files.Count)];
				long startTime = org.apache.hadoop.util.Time.now();
				java.io.InputStream @in = org.apache.hadoop.fs.loadGenerator.LoadGenerator.fc.open
					(new org.apache.hadoop.fs.Path(fileName));
				this.executionTime[org.apache.hadoop.fs.loadGenerator.LoadGenerator.OPEN] += (org.apache.hadoop.util.Time
					.now() - startTime);
				this.totalNumOfOps[org.apache.hadoop.fs.loadGenerator.LoadGenerator.OPEN]++;
				while (@in.read(this.buffer) != -1)
				{
				}
				@in.close();
			}

			/// <summary>
			/// The write operation randomly picks a directory in the
			/// test space and creates a file whose name consists of the current
			/// machine's host name and the thread id.
			/// </summary>
			/// <remarks>
			/// The write operation randomly picks a directory in the
			/// test space and creates a file whose name consists of the current
			/// machine's host name and the thread id. The length of the file
			/// follows Gaussian distribution with an average size of 2 blocks and
			/// the standard deviation of 1 block. The new file is filled with 'a'.
			/// Immediately after the file creation completes, the file is deleted
			/// from the test space.
			/// </remarks>
			/// <exception cref="System.IO.IOException"/>
			private void write()
			{
				string dirName = org.apache.hadoop.fs.loadGenerator.LoadGenerator.dirs[org.apache.hadoop.fs.loadGenerator.LoadGenerator
					.r.nextInt(org.apache.hadoop.fs.loadGenerator.LoadGenerator.dirs.Count)];
				org.apache.hadoop.fs.Path file = new org.apache.hadoop.fs.Path(dirName, this._enclosing
					.hostname + this.id);
				double fileSize = 0;
				while ((fileSize = org.apache.hadoop.fs.loadGenerator.LoadGenerator.r.nextGaussian
					() + 2) <= 0)
				{
				}
				this.genFile(file, (long)(fileSize * org.apache.hadoop.fs.loadGenerator.LoadGenerator
					.BLOCK_SIZE));
				long startTime = org.apache.hadoop.util.Time.now();
				org.apache.hadoop.fs.loadGenerator.LoadGenerator.fc.delete(file, true);
				this.executionTime[org.apache.hadoop.fs.loadGenerator.LoadGenerator.DELETE] += (org.apache.hadoop.util.Time
					.now() - startTime);
				this.totalNumOfOps[org.apache.hadoop.fs.loadGenerator.LoadGenerator.DELETE]++;
			}

			/// <summary>
			/// The list operation randomly picks a directory in the test space and
			/// list the directory content.
			/// </summary>
			/// <exception cref="System.IO.IOException"/>
			private void list()
			{
				string dirName = org.apache.hadoop.fs.loadGenerator.LoadGenerator.dirs[org.apache.hadoop.fs.loadGenerator.LoadGenerator
					.r.nextInt(org.apache.hadoop.fs.loadGenerator.LoadGenerator.dirs.Count)];
				long startTime = org.apache.hadoop.util.Time.now();
				org.apache.hadoop.fs.loadGenerator.LoadGenerator.fc.listStatus(new org.apache.hadoop.fs.Path
					(dirName));
				this.executionTime[org.apache.hadoop.fs.loadGenerator.LoadGenerator.LIST] += (org.apache.hadoop.util.Time
					.now() - startTime);
				this.totalNumOfOps[org.apache.hadoop.fs.loadGenerator.LoadGenerator.LIST]++;
			}

			/// <summary>Create a file with a length of <code>fileSize</code>.</summary>
			/// <remarks>
			/// Create a file with a length of <code>fileSize</code>.
			/// The file is filled with 'a'.
			/// </remarks>
			/// <exception cref="System.IO.IOException"/>
			private void genFile(org.apache.hadoop.fs.Path file, long fileSize)
			{
				long startTime = org.apache.hadoop.util.Time.now();
				org.apache.hadoop.fs.FSDataOutputStream @out = null;
				try
				{
					@out = org.apache.hadoop.fs.loadGenerator.LoadGenerator.fc.create(file, java.util.EnumSet
						.of(org.apache.hadoop.fs.CreateFlag.CREATE, org.apache.hadoop.fs.CreateFlag.OVERWRITE
						), org.apache.hadoop.fs.Options.CreateOpts.createParent(), org.apache.hadoop.fs.Options.CreateOpts
						.bufferSize(4096), org.apache.hadoop.fs.Options.CreateOpts.repFac((short)3));
					this.executionTime[org.apache.hadoop.fs.loadGenerator.LoadGenerator.CREATE] += (org.apache.hadoop.util.Time
						.now() - startTime);
					org.apache.hadoop.fs.loadGenerator.LoadGenerator.numOfOps[org.apache.hadoop.fs.loadGenerator.LoadGenerator
						.CREATE]++;
					long i = fileSize;
					while (i > 0)
					{
						long s = System.Math.min(fileSize, this._enclosing.WRITE_CONTENTS.Length);
						@out.write(this._enclosing.WRITE_CONTENTS, 0, (int)s);
						i -= s;
					}
					startTime = org.apache.hadoop.util.Time.now();
					this.executionTime[org.apache.hadoop.fs.loadGenerator.LoadGenerator.WRITE_CLOSE] 
						+= (org.apache.hadoop.util.Time.now() - startTime);
					org.apache.hadoop.fs.loadGenerator.LoadGenerator.numOfOps[org.apache.hadoop.fs.loadGenerator.LoadGenerator
						.WRITE_CLOSE]++;
				}
				finally
				{
					org.apache.hadoop.io.IOUtils.cleanup(org.apache.hadoop.fs.loadGenerator.LoadGenerator
						.LOG, @out);
				}
			}

			private readonly LoadGenerator _enclosing;
		}

		/// <summary>Main function called by tool runner.</summary>
		/// <remarks>
		/// Main function called by tool runner.
		/// It first initializes data by parsing the command line arguments.
		/// It then calls the loadGenerator
		/// </remarks>
		/// <exception cref="System.Exception"/>
		public virtual int run(string[] args)
		{
			int exitCode = parseArgs(false, args);
			if (exitCode != 0)
			{
				return exitCode;
			}
			System.Console.Out.WriteLine("Running LoadGenerator against fileSystem: " + org.apache.hadoop.fs.FileContext
				.getFileContext().getDefaultFileSystem().getUri());
			exitCode = generateLoadOnNN();
			printResults(System.Console.Out);
			return exitCode;
		}

		internal virtual bool stopFileCreated()
		{
			try
			{
				fc.getFileStatus(flagFile);
			}
			catch (java.io.FileNotFoundException)
			{
				return false;
			}
			catch (System.IO.IOException e)
			{
				LOG.error("Got error when checking if file exists:" + flagFile, e);
			}
			LOG.info("Flag file was created. Stopping the test.");
			return true;
		}

		/// <summary>
		/// This is the main function - run threads to generate load on NN
		/// It starts the number of DFSClient threads as specified by
		/// the user.
		/// </summary>
		/// <remarks>
		/// This is the main function - run threads to generate load on NN
		/// It starts the number of DFSClient threads as specified by
		/// the user.
		/// It stops all the threads when the specified elapsed time is passed.
		/// </remarks>
		/// <exception cref="System.Exception"/>
		protected internal virtual int generateLoadOnNN()
		{
			int hostHashCode = hostname.GetHashCode();
			if (seed == 0)
			{
				r = new java.util.Random(Sharpen.Runtime.currentTimeMillis() + hostHashCode);
			}
			else
			{
				r = new java.util.Random(seed + hostHashCode);
			}
			try
			{
				fc = org.apache.hadoop.fs.FileContext.getFileContext(getConf());
			}
			catch (System.IO.IOException ioe)
			{
				System.Console.Error.WriteLine("Can not initialize the file system: " + ioe.getLocalizedMessage
					());
				return -1;
			}
			int status = initFileDirTables();
			if (status != 0)
			{
				return status;
			}
			barrier();
			org.apache.hadoop.fs.loadGenerator.LoadGenerator.DFSClientThread[] threads = new 
				org.apache.hadoop.fs.loadGenerator.LoadGenerator.DFSClientThread[numOfThreads];
			for (int i = 0; i < numOfThreads; i++)
			{
				threads[i] = new org.apache.hadoop.fs.loadGenerator.LoadGenerator.DFSClientThread
					(this, i);
				threads[i].start();
			}
			if (durations[0] > 0)
			{
				if (durations.Length == 1)
				{
					// There is a fixed run time
					while (shouldRun)
					{
						java.lang.Thread.sleep(2000);
						totalTime += 2;
						if (totalTime >= durations[0] || stopFileCreated())
						{
							shouldRun = false;
						}
					}
				}
				else
				{
					// script run
					while (shouldRun)
					{
						java.lang.Thread.sleep(durations[currentIndex] * 1000);
						totalTime += durations[currentIndex];
						// Are we on the final line of the script?
						if ((currentIndex + 1) == durations.Length || stopFileCreated())
						{
							shouldRun = false;
						}
						else
						{
							if (LOG.isDebugEnabled())
							{
								LOG.debug("Moving to index " + currentIndex + ": r = " + readProbs[currentIndex] 
									+ ", w = " + writeProbs + " for duration " + durations[currentIndex]);
							}
							currentIndex++;
						}
					}
				}
			}
			if (LOG.isDebugEnabled())
			{
				LOG.debug("Done with testing.  Waiting for threads to finish.");
			}
			bool failed = false;
			foreach (org.apache.hadoop.fs.loadGenerator.LoadGenerator.DFSClientThread thread in 
				threads)
			{
				thread.join();
				for (int i_1 = 0; i_1 < TOTAL_OP_TYPES; i_1++)
				{
					executionTime[i_1] += thread.executionTime[i_1];
					numOfOps[i_1] += thread.totalNumOfOps[i_1];
				}
				failed = failed || thread.failed;
			}
			int exitCode = 0;
			if (failed)
			{
				exitCode = -ERR_TEST_FAILED;
			}
			totalOps = 0;
			for (int i_2 = 0; i_2 < TOTAL_OP_TYPES; i_2++)
			{
				totalOps += numOfOps[i_2];
			}
			return exitCode;
		}

		/// <exception cref="org.apache.hadoop.fs.UnsupportedFileSystemException"/>
		protected internal static void printResults(System.IO.TextWriter @out)
		{
			@out.WriteLine("Result of running LoadGenerator against fileSystem: " + org.apache.hadoop.fs.FileContext
				.getFileContext().getDefaultFileSystem().getUri());
			if (numOfOps[OPEN] != 0)
			{
				@out.WriteLine("Average open execution time: " + (double)executionTime[OPEN] / numOfOps
					[OPEN] + "ms");
			}
			if (numOfOps[LIST] != 0)
			{
				@out.WriteLine("Average list execution time: " + (double)executionTime[LIST] / numOfOps
					[LIST] + "ms");
			}
			if (numOfOps[DELETE] != 0)
			{
				@out.WriteLine("Average deletion execution time: " + (double)executionTime[DELETE
					] / numOfOps[DELETE] + "ms");
				@out.WriteLine("Average create execution time: " + (double)executionTime[CREATE] 
					/ numOfOps[CREATE] + "ms");
				@out.WriteLine("Average write_close execution time: " + (double)executionTime[WRITE_CLOSE
					] / numOfOps[WRITE_CLOSE] + "ms");
			}
			if (totalTime != 0)
			{
				@out.WriteLine("Average operations per second: " + (double)totalOps / totalTime +
					 "ops/s");
			}
			@out.WriteLine();
		}

		/// <summary>Parse the command line arguments and initialize the data</summary>
		/// <exception cref="System.IO.IOException"/>
		protected internal virtual int parseArgs(bool runAsMapReduce, string[] args)
		{
			try
			{
				for (int i = 0; i < args.Length; i++)
				{
					// parse command line
					if (args[i].Equals("-scriptFile"))
					{
						scriptFile = args[++i];
						if (durations[0] > 0)
						{
							System.Console.Error.WriteLine("Can't specify elapsedTime and use script.");
							return -1;
						}
					}
					else
					{
						if (args[i].Equals("-readProbability"))
						{
							if (scriptFile != null)
							{
								System.Console.Error.WriteLine("Can't specify probabilities and use script.");
								return -1;
							}
							readProbs[0] = double.parseDouble(args[++i]);
							if (readProbs[0] < 0 || readProbs[0] > 1)
							{
								System.Console.Error.WriteLine("The read probability must be [0, 1]: " + readProbs
									[0]);
								return -1;
							}
						}
						else
						{
							if (args[i].Equals("-writeProbability"))
							{
								if (scriptFile != null)
								{
									System.Console.Error.WriteLine("Can't specify probabilities and use script.");
									return -1;
								}
								writeProbs[0] = double.parseDouble(args[++i]);
								if (writeProbs[0] < 0 || writeProbs[0] > 1)
								{
									System.Console.Error.WriteLine("The write probability must be [0, 1]: " + writeProbs
										[0]);
									return -1;
								}
							}
							else
							{
								if (args[i].Equals("-root"))
								{
									root = new org.apache.hadoop.fs.Path(args[++i]);
								}
								else
								{
									if (args[i].Equals("-maxDelayBetweenOps"))
									{
										maxDelayBetweenOps = System.Convert.ToInt32(args[++i]);
									}
									else
									{
										// in milliseconds
										if (args[i].Equals("-numOfThreads"))
										{
											numOfThreads = System.Convert.ToInt32(args[++i]);
											if (numOfThreads <= 0)
											{
												System.Console.Error.WriteLine("Number of threads must be positive: " + numOfThreads
													);
												return -1;
											}
										}
										else
										{
											if (args[i].Equals("-startTime"))
											{
												startTime = long.Parse(args[++i]);
											}
											else
											{
												if (args[i].Equals("-elapsedTime"))
												{
													if (scriptFile != null)
													{
														System.Console.Error.WriteLine("Can't specify elapsedTime and use script.");
														return -1;
													}
													durations[0] = long.Parse(args[++i]);
												}
												else
												{
													if (args[i].Equals("-seed"))
													{
														seed = long.Parse(args[++i]);
														r = new java.util.Random(seed);
													}
													else
													{
														if (args[i].Equals("-flagFile"))
														{
															LOG.info("got flagFile:" + flagFile);
															flagFile = new org.apache.hadoop.fs.Path(args[++i]);
														}
														else
														{
															System.Console.Error.WriteLine(USAGE);
															org.apache.hadoop.util.ToolRunner.printGenericCommandUsage(System.Console.Error);
															return -1;
														}
													}
												}
											}
										}
									}
								}
							}
						}
					}
				}
			}
			catch (java.lang.NumberFormatException e)
			{
				System.Console.Error.WriteLine("Illegal parameter: " + e.getLocalizedMessage());
				System.Console.Error.WriteLine(USAGE);
				return -1;
			}
			// Load Script File if not MR; for MR scriptFile is loaded by Mapper
			if (!runAsMapReduce && scriptFile != null)
			{
				if (loadScriptFile(scriptFile, true) == -1)
				{
					return -1;
				}
			}
			for (int i_1 = 0; i_1 < readProbs.Length; i_1++)
			{
				if (readProbs[i_1] + writeProbs[i_1] < 0 || readProbs[i_1] + writeProbs[i_1] > 1)
				{
					System.Console.Error.WriteLine("The sum of read probability and write probability must be [0, 1]: "
						 + readProbs[i_1] + " " + writeProbs[i_1]);
					return -1;
				}
			}
			return 0;
		}

		private static void parseScriptLine(string line, System.Collections.Generic.List<
			long> duration, System.Collections.Generic.List<double> readProb, System.Collections.Generic.List
			<double> writeProb)
		{
			string[] a = line.split("\\s");
			if (a.Length != 3)
			{
				throw new System.ArgumentException("Incorrect number of parameters: " + line);
			}
			try
			{
				long d = long.Parse(a[0]);
				double r = double.parseDouble(a[1]);
				double w = double.parseDouble(a[2]);
				com.google.common.@base.Preconditions.checkArgument(d >= 0, "Invalid duration: " 
					+ d);
				com.google.common.@base.Preconditions.checkArgument(0 <= r && r <= 1.0, "The read probability must be [0, 1]: "
					 + r);
				com.google.common.@base.Preconditions.checkArgument(0 <= w && w <= 1.0, "The read probability must be [0, 1]: "
					 + w);
				readProb.add(r);
				duration.add(d);
				writeProb.add(w);
			}
			catch (java.lang.NumberFormatException)
			{
				throw new System.ArgumentException("Cannot parse: " + line);
			}
		}

		/// <summary>
		/// Read a script file of the form: lines of text with duration in seconds,
		/// read probability and write probability, separated by white space.
		/// </summary>
		/// <param name="filename">Script file</param>
		/// <returns>0 if successful, -1 if not</returns>
		/// <exception cref="System.IO.IOException">if errors with file IO</exception>
		protected internal static int loadScriptFile(string filename, bool readLocally)
		{
			org.apache.hadoop.fs.FileContext fc;
			if (readLocally)
			{
				// read locally - program is run without MR
				fc = org.apache.hadoop.fs.FileContext.getLocalFSFileContext();
			}
			else
			{
				fc = org.apache.hadoop.fs.FileContext.getFileContext();
			}
			// use default file system
			java.io.DataInputStream @in = null;
			try
			{
				@in = fc.open(new org.apache.hadoop.fs.Path(filename));
			}
			catch (System.IO.IOException)
			{
				System.Console.Error.WriteLine("Unable to open scriptFile: " + filename);
				System.Environment.Exit(-1);
			}
			java.io.InputStreamReader inr = new java.io.InputStreamReader(@in);
			java.io.BufferedReader br = new java.io.BufferedReader(inr);
			System.Collections.Generic.List<long> duration = new System.Collections.Generic.List
				<long>();
			System.Collections.Generic.List<double> readProb = new System.Collections.Generic.List
				<double>();
			System.Collections.Generic.List<double> writeProb = new System.Collections.Generic.List
				<double>();
			int lineNum = 0;
			string line;
			// Read script, parse values, build array of duration, read and write probs
			try
			{
				while ((line = br.readLine()) != null)
				{
					lineNum++;
					if (line.StartsWith("#") || line.isEmpty())
					{
						// skip comments and blanks
						continue;
					}
					parseScriptLine(line, duration, readProb, writeProb);
				}
			}
			catch (System.ArgumentException e)
			{
				System.Console.Error.WriteLine("Line: " + lineNum + ", " + e.Message);
				return -1;
			}
			finally
			{
				org.apache.hadoop.io.IOUtils.cleanup(LOG, br);
			}
			// Copy vectors to arrays of values, to avoid autoboxing overhead later
			durations = new long[duration.Count];
			readProbs = new double[readProb.Count];
			writeProbs = new double[writeProb.Count];
			for (int i = 0; i < durations.Length; i++)
			{
				durations[i] = duration[i];
				readProbs[i] = readProb[i];
				writeProbs[i] = writeProb[i];
			}
			if (durations[0] == 0)
			{
				System.Console.Error.WriteLine("Initial duration set to 0.  " + "Will loop until stopped manually."
					);
			}
			return 0;
		}

		/// <summary>
		/// Create a table that contains all directories under root and
		/// another table that contains all files under root.
		/// </summary>
		private int initFileDirTables()
		{
			try
			{
				initFileDirTables(root);
			}
			catch (System.IO.IOException e)
			{
				System.Console.Error.WriteLine(e.getLocalizedMessage());
				Sharpen.Runtime.printStackTrace(e);
				return -1;
			}
			if (dirs.isEmpty())
			{
				System.Console.Error.WriteLine("The test space " + root + " is empty");
				return -1;
			}
			if (files.isEmpty())
			{
				System.Console.Error.WriteLine("The test space " + root + " does not have any file"
					);
				return -1;
			}
			return 0;
		}

		/// <summary>
		/// Create a table that contains all directories under the specified path and
		/// another table that contains all files under the specified path and
		/// whose name starts with "_file_".
		/// </summary>
		/// <exception cref="System.IO.IOException"/>
		private void initFileDirTables(org.apache.hadoop.fs.Path path)
		{
			org.apache.hadoop.fs.FileStatus[] stats = fc.util().listStatus(path);
			foreach (org.apache.hadoop.fs.FileStatus stat in stats)
			{
				if (stat.isDirectory())
				{
					dirs.add(stat.getPath().ToString());
					initFileDirTables(stat.getPath());
				}
				else
				{
					org.apache.hadoop.fs.Path filePath = stat.getPath();
					if (filePath.getName().StartsWith(org.apache.hadoop.fs.loadGenerator.StructureGenerator
						.FILE_NAME_PREFIX))
					{
						files.add(filePath.ToString());
					}
				}
			}
		}

		/// <summary>
		/// Returns when the current number of seconds from the epoch equals
		/// the command line argument given by <code>-startTime</code>.
		/// </summary>
		/// <remarks>
		/// Returns when the current number of seconds from the epoch equals
		/// the command line argument given by <code>-startTime</code>.
		/// This allows multiple instances of this program, running on clock
		/// synchronized nodes, to start at roughly the same time.
		/// </remarks>
		private static void barrier()
		{
			long sleepTime;
			while ((sleepTime = startTime - org.apache.hadoop.util.Time.now()) > 0)
			{
				try
				{
					java.lang.Thread.sleep(sleepTime);
				}
				catch (System.Exception)
				{
				}
			}
		}

		/// <summary>Main program</summary>
		/// <param name="args">command line arguments</param>
		/// <exception cref="System.Exception"/>
		public static void Main(string[] args)
		{
			int res = org.apache.hadoop.util.ToolRunner.run(new org.apache.hadoop.conf.Configuration
				(), new org.apache.hadoop.fs.loadGenerator.LoadGenerator(), args);
			System.Environment.Exit(res);
		}
	}
}
