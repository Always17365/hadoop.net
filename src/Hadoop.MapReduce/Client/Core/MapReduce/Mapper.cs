using System;
using Org.Apache.Hadoop.Conf;
using Org.Apache.Hadoop.FS;
using Org.Apache.Hadoop.IO;
using Org.Apache.Hadoop.Security;
using Sharpen;

namespace Org.Apache.Hadoop.Mapreduce
{
	/// <summary>Maps input key/value pairs to a set of intermediate key/value pairs.</summary>
	/// <remarks>
	/// Maps input key/value pairs to a set of intermediate key/value pairs.
	/// <p>Maps are the individual tasks which transform input records into a
	/// intermediate records. The transformed intermediate records need not be of
	/// the same type as the input records. A given input pair may map to zero or
	/// many output pairs.</p>
	/// <p>The Hadoop Map-Reduce framework spawns one map task for each
	/// <see cref="InputSplit"/>
	/// generated by the
	/// <see cref="InputFormat{K, V}"/>
	/// for the job.
	/// <code>Mapper</code> implementations can access the
	/// <see cref="Org.Apache.Hadoop.Conf.Configuration"/>
	/// for
	/// the job via the
	/// <see cref="JobContext.GetConfiguration()"/>
	/// .
	/// <p>The framework first calls
	/// <see cref="Mapper{KEYIN, VALUEIN, KEYOUT, VALUEOUT}.Setup(Context)"/>
	/// , followed by
	/// <see cref="Mapper{KEYIN, VALUEIN, KEYOUT, VALUEOUT}.Map(object, object, Context)"
	/// 	/>
	/// for each key/value pair in the <code>InputSplit</code>. Finally
	/// <see cref="Mapper{KEYIN, VALUEIN, KEYOUT, VALUEOUT}.Cleanup(Context)"/>
	/// is called.</p>
	/// <p>All intermediate values associated with a given output key are
	/// subsequently grouped by the framework, and passed to a
	/// <see cref="Reducer{KEYIN, VALUEIN, KEYOUT, VALUEOUT}"/>
	/// to
	/// determine the final output. Users can control the sorting and grouping by
	/// specifying two key
	/// <see cref="Org.Apache.Hadoop.IO.RawComparator{T}"/>
	/// classes.</p>
	/// <p>The <code>Mapper</code> outputs are partitioned per
	/// <code>Reducer</code>. Users can control which keys (and hence records) go to
	/// which <code>Reducer</code> by implementing a custom
	/// <see cref="Partitioner{KEY, VALUE}"/>
	/// .
	/// <p>Users can optionally specify a <code>combiner</code>, via
	/// <see cref="Job.SetCombinerClass(System.Type{T})"/>
	/// , to perform local aggregation of the
	/// intermediate outputs, which helps to cut down the amount of data transferred
	/// from the <code>Mapper</code> to the <code>Reducer</code>.
	/// <p>Applications can specify if and how the intermediate
	/// outputs are to be compressed and which
	/// <see cref="Org.Apache.Hadoop.IO.Compress.CompressionCodec"/>
	/// s are to be
	/// used via the <code>Configuration</code>.</p>
	/// <p>If the job has zero
	/// reduces then the output of the <code>Mapper</code> is directly written
	/// to the
	/// <see cref="OutputFormat{K, V}"/>
	/// without sorting by keys.</p>
	/// <p>Example:</p>
	/// <p><blockquote><pre>
	/// public class TokenCounterMapper
	/// extends Mapper&lt;Object, Text, Text, IntWritable&gt;{
	/// private final static IntWritable one = new IntWritable(1);
	/// private Text word = new Text();
	/// public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
	/// StringTokenizer itr = new StringTokenizer(value.toString());
	/// while (itr.hasMoreTokens()) {
	/// word.set(itr.nextToken());
	/// context.write(word, one);
	/// }
	/// }
	/// }
	/// </pre></blockquote>
	/// <p>Applications may override the
	/// <see cref="Mapper{KEYIN, VALUEIN, KEYOUT, VALUEOUT}.Run(Context)"/>
	/// method to exert
	/// greater control on map processing e.g. multi-threaded <code>Mapper</code>s
	/// etc.</p>
	/// </remarks>
	/// <seealso cref="InputFormat{K, V}"/>
	/// <seealso cref="JobContext"/>
	/// <seealso cref="Partitioner{KEY, VALUE}"></seealso>
	/// <seealso cref="Reducer{KEYIN, VALUEIN, KEYOUT, VALUEOUT}"/>
	public class Mapper<Keyin, Valuein, Keyout, Valueout>
	{
		/// <summary>
		/// The <code>Context</code> passed on to the
		/// <see cref="Mapper{KEYIN, VALUEIN, KEYOUT, VALUEOUT}"/>
		/// implementations.
		/// </summary>
		public abstract class Context : MapContext<KEYIN, VALUEIN, KEYOUT, VALUEOUT>
		{
			public abstract Path[] GetArchiveClassPaths();

			public abstract string[] GetArchiveTimestamps();

			public abstract URI[] GetCacheArchives();

			public abstract URI[] GetCacheFiles();

			public abstract Type GetCombinerClass();

			public abstract RawComparator<object> GetCombinerKeyGroupingComparator();

			public abstract Configuration GetConfiguration();

			public abstract Credentials GetCredentials();

			public abstract Path[] GetFileClassPaths();

			public abstract string[] GetFileTimestamps();

			public abstract RawComparator<object> GetGroupingComparator();

			public abstract Type GetInputFormatClass();

			public abstract string GetJar();

			public abstract JobID GetJobID();

			public abstract string GetJobName();

			public abstract bool GetJobSetupCleanupNeeded();

			public abstract Path[] GetLocalCacheArchives();

			public abstract Path[] GetLocalCacheFiles();

			public abstract Type GetMapOutputKeyClass();

			public abstract Type GetMapOutputValueClass();

			public abstract Type GetMapperClass();

			public abstract int GetMaxMapAttempts();

			public abstract int GetMaxReduceAttempts();

			public abstract int GetNumReduceTasks();

			public abstract Type GetOutputFormatClass();

			public abstract Type GetOutputKeyClass();

			public abstract Type GetOutputValueClass();

			public abstract Type GetPartitionerClass();

			public abstract bool GetProfileEnabled();

			public abstract string GetProfileParams();

			public abstract Configuration.IntegerRanges GetProfileTaskRange(bool arg1);

			public abstract Type GetReducerClass();

			public abstract RawComparator<object> GetSortComparator();

			public abstract bool GetSymlink();

			public abstract bool GetTaskCleanupNeeded();

			public abstract string GetUser();

			public abstract Path GetWorkingDirectory();

			public abstract void Progress();

			public abstract Counter GetCounter(Enum<object> arg1);

			public abstract Counter GetCounter(string arg1, string arg2);

			public abstract float GetProgress();

			public abstract string GetStatus();

			public abstract TaskAttemptID GetTaskAttemptID();

			public abstract void SetStatus(string arg1);

			public abstract KEYIN GetCurrentKey();

			public abstract VALUEIN GetCurrentValue();

			public abstract OutputCommitter GetOutputCommitter();

			public abstract bool NextKeyValue();

			public abstract void Write(KEYOUT arg1, VALUEOUT arg2);

			public abstract InputSplit GetInputSplit();

			internal Context(Mapper<Keyin, Valuein, Keyout, Valueout> _enclosing)
			{
				this._enclosing = _enclosing;
			}

			private readonly Mapper<Keyin, Valuein, Keyout, Valueout> _enclosing;
		}

		/// <summary>Called once at the beginning of the task.</summary>
		/// <exception cref="System.IO.IOException"/>
		/// <exception cref="System.Exception"/>
		protected internal virtual void Setup(Mapper.Context context)
		{
		}

		// NOTHING
		/// <summary>Called once for each key/value pair in the input split.</summary>
		/// <remarks>
		/// Called once for each key/value pair in the input split. Most applications
		/// should override this, but the default is the identity function.
		/// </remarks>
		/// <exception cref="System.IO.IOException"/>
		/// <exception cref="System.Exception"/>
		protected internal virtual void Map(KEYIN key, VALUEIN value, Mapper.Context context
			)
		{
			context.Write((KEYOUT)key, (VALUEOUT)value);
		}

		/// <summary>Called once at the end of the task.</summary>
		/// <exception cref="System.IO.IOException"/>
		/// <exception cref="System.Exception"/>
		protected internal virtual void Cleanup(Mapper.Context context)
		{
		}

		// NOTHING
		/// <summary>
		/// Expert users can override this method for more complete control over the
		/// execution of the Mapper.
		/// </summary>
		/// <param name="context"/>
		/// <exception cref="System.IO.IOException"/>
		/// <exception cref="System.Exception"/>
		public virtual void Run(Mapper.Context context)
		{
			Setup(context);
			try
			{
				while (context.NextKeyValue())
				{
					Map(context.GetCurrentKey(), context.GetCurrentValue(), context);
				}
			}
			finally
			{
				Cleanup(context);
			}
		}
	}
}
